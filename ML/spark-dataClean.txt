# Import the pyspark.sql.types library
from pyspark.sql.types import *

# Define a new schema using the StructType method
people_schema = StructType([
  # Define a StructField for each field
  StructField('name', StringType(), False),
  StructField('age', IntegerType(), False),
  StructField('city', StringType(), False)
])

------------------------Using lazy processing--------------------------------------------

Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. 
Remember that this is due to Spark not performing any transformations until an action is requested.

# Load the CSV file
aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')

# Add the airport column using the F.lower() method
aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))

# Drop the Destination Airport column
aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])

# Show the DataFrame
aa_dfw_df.show()

----------------------------Saving a DataFrame in Parquet format-----------------------------------------------------------------

When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, 
but it is not an optimal format for Spark. The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. 
This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. 
This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.

# View the row count of df1 and df2
print("df1 Count: %d" % df1.count())
print("df2 Count: %d" % df2.count())

# Combine the DataFrames into one 
df3 = df1.union(df2)

# Save the df3 DataFrame in Parquet format
df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')

# Read the Parquet file into a new DataFrame and run a count
print(spark.read.parquet('AA_DFW_ALL.parquet').count())

-------------------------------------------SQL and Parquet-------------------------------------------------------------------------------------
Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, 
sometimes it's easier to run SQL queries alongside the Python options.

# Read the Parquet file into flights_df
flights_df = spark.read.parquet('AA_DFW_ALL.parquet')

# Register the temp table
flights_df.createOrReplaceTempView('flights')

# Run a SQL query of the average flight duration
avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]
print('The average flight time is: %d' % avg_duration)

----------------------------------------------Filtering column content with Python---------------------------------------------------------------------------------------------------------
You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame voter_df contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.

This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the VOTER_NAME column.

The pyspark.sql.functions library is already imported under the alias F

# Show the distinct VOTER_NAME entries
voter_df.select(voter_df['VOTER_NAME']).distinct().show(40, truncate=False)

# Filter voter_df where the VOTER_NAME is 1-20 characters in length
voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')

# Filter out voter_df where the VOTER_NAME contains an underscore
voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))

# Show the distinct VOTER_NAME entries again
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)

---------------------------Filtering Question #1--------------------------------------------------------------------------------------------------

Consider the following Data Frame called users_df:

ID	Name	Age	State
140	George  47	Iowa
3260	Mary R	34	Vermont
18502	null	68	Ohio
999	Rick W	23	California
If you wanted to return only the entries without nulls, which of following options would not work?

users_df = users_df.filter(users_df.Name.isNotNull())
users_df = users_df.where(~ users_df.ID = 18502 )
users_df = users_df.filter(~ col('Name').isNull())

all are working but the following
users_df = users_df.where(users_df.ID = 18502)

--------------------------------Filtering Question #2--------------------------------
Consider the following Data Frame called users_df:

ID	Name	Age	State
140	George L	47	Iowa
3260	Mary R	34	Vermont
18502	Audrey V	68	Ohio
999	Rick W	23	California
If we wanted to return only the Name and State fields for any ID greater than 3000, which code snippet meets these requirements?

users_df.filter('ID > 3000').select("Name", "State")

----------------------------------Modifying DataFrame columns--------------------------------------------------------------------
-- Always refer to the Spark documentation when you need to modify a string column: 

# Add a new column called splits separated on whitespace
voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\s+'))

# Create a new column called first_name based on the first item in splits
voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))

# Get the last entry of the splits list and create a column called last_name
voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))

# Drop the splits column
voter_df = voter_df.drop('splits')

# Show the voter_df DataFrame
voter_df.show()

----------------------------------------when() example------------------------------------------------------------------------------

# Add a column to voter_df for any voter with the title **Councilmember**
voter_df = voter_df.withColumn('random_val',
                               when(voter_df.TITLE == 'Councilmember', F.rand()))

# Show some of the DataFrame rows, noting whether the when clause worked
voter_df.show()

-------------------------------When / Otherwise-------------------------------------------------------------------------------------------

# Add a column to voter_df for a voter based on their position
voter_df = voter_df.withColumn('random_val',
                               when(voter_df.TITLE == 'Councilmember', F.rand())
                               .when(voter_df.TITLE == 'Mayor', 2)
                               .otherwise(0))

# Show some of the DataFrame rows
voter_df.show()

# Use the .filter() clause with random_val
voter_df.filter(voter_df.random_val == 0).show()

---------------------------------------Using user defined functions in Spark--------------------------------------------------------------------------------------------------
You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, 
it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions 
to manipulate our DataFrames.

def getFirstAndMiddle(names):
  # Return a space separated string of names
  return ' '.join(names[:-1])

# Define the method as a UDF
udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())

# Create a new column using your UDF
voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))

# Drop the unecessary columns then show the DataFrame
voter_df = voter_df.drop('first_name')
voter_df = voter_df.drop('splits')
voter_df.show()

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Adding an ID Field
When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame 
and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than 
the actual number of rows in the DataFrame.

With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset.


# Select all the unique council voters
voter_df = df.select(df["VOTER NAME"]).distinct()

# Count the rows in voter_df
print("\nThere are %d rows in the voter_df DataFrame.\n" % voter_df.count())

# Add a ROW_ID
voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())

# Show the rows with 10 highest IDs in the set
voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)

-----------------------Adding an ID Field-----------------------------------------------------------------------------------------------------------------------------------------
When working with data, you sometimes only want to access certain fields and perform various operations. 
In this case, find all the unique voter names from the DataFrame and add a unique ID number. 
Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.

# Select all the unique council voters
voter_df = df.select(df["VOTER NAME"]).distinct()

# Count the rows in voter_df
print("\nThere are %d rows in the voter_df DataFrame.\n" % voter_df.count())

# Add a ROW_ID
voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())

# Show the rows with 10 highest IDs in the set
voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)

----------------------------IDs with different partitions------------------------------------------------------------------------------------------------------------

# Print the number of partitions in each DataFrame
print("\nThere are %d partitions in the voter_df DataFrame.\n" % voter_df.rdd.getNumPartitions())
print("\nThere are %d partitions in the voter_df_single DataFrame.\n" % voter_df_single.rdd.getNumPartitions())

# Add a ROW_ID field to each DataFrame
voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())
voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())

# Show the top 10 IDs in each DataFrame 
voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)
voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)

---------------------------------------More ID tricks---------------------------------------------------------------------------------------------------------------------
Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, 
you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. 
This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output 
from a monthly Spark task start at the highest value from the previous month.

# Determine the highest ROW_ID and save it in previous_max_ID
previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]

# Add a ROW_ID column to voter_df_april starting at the desired value
voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)

# Show the ROW_ID from both DataFrames and compare
voter_df_march.select('ROW_ID').show()
voter_df_april.select('ROW_ID').show()

------------------------------------Caching a DataFrame-----------------------------------------------------------------------------------------------
You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames 
and would like to implement it.

start_time = time.time()

# Add caching to the unique rows in departures_df
departures_df = departures_df.distinct().cache()

# Count the unique rows in departures_df, noting how long the operation takes
print("Counting %d rows took %f seconds" % (departures_df.count(), time.time() - start_time))

# Count the rows again, noting the variance in time of a cached DataFrame
start_time = time.time()
print("Counting %d rows again took %f seconds" % (departures_df.count(), time.time() - start_time))

------------------------------------Removing a DataFrame from cache-------------------------------------------------------------------------
# Determine if departures_df is in the cache
print("Is departures_df cached?: %s" % departures_df.is_cached)
print("Removing departures_df from cache")

# Remove departures_df from the cache
departures_df.unpersist()

# Check the cache status again
print("Is departures_df cached?: %s" % departures_df.is_cached)

------------------------------------File size optimization------------------------------------------------------------------------------------------------

Consider if you're given 2 large data files on a cluster with 10 nodes. Each file contains 10M rows of roughly the same size. 
While working with your data, the responsiveness is acceptable but the initial read from the files takes a considerable period of time. 
Note that you are the only one who will use the data and it changes for each run.

Which of the following is the best option to improve performance?

Split the 2 files into 50 files of 400K rows each.

---------------------------------------File import performance----------------------------------------------------------------------------------------------------

# Import the full and split files into DataFrames
full_df = spark.read.csv('departures_full.txt.gz')
split_df = spark.read.csv('departures_0*.txt.gz')

# Print the count and run time for each DataFrame
start_time_a = time.time()
print("Total rows in full DataFrame:\t%d" % full_df.count())
print("Time to run: %f" % (time.time() - start_time_a))

start_time_b = time.time()
print("Total rows in split DataFrame:\t%d" % split_df.count())
print("Time to run: %f" % (time.time() - start_time_b))

-------------------------------------Reading Spark configurations----------------------------------------------------------------------------------------------------------------------------------

# Name of the Spark application instance
app_name = spark.conf.get('spark.app.name')

# Driver TCP port
driver_tcp_port = spark.conf.get('spark.driver.port')

# Number of join partitions
num_partitions = spark.conf.get('spark.sql.shuffle.partitions')

# Show the results
print("Name: %s" % app_name)
print("Driver TCP port: %s" % driver_tcp_port)
print("Number of partitions: %s" % num_partitions)

-------------------------------------Writing Spark configurations---------------------------------------------------------------------------------------------------------------

Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. 
You'll import some data to review that your changes have affected the cluster.

The spark configuration is initially set to the default value of 200 partitions.

The spark object is available for use. A file named departures.txt.gz is available for import. 
An initial DataFrame containing the distinct rows from departures.txt.gz is available as departures_df

# Store the number of partitions in variable
before = departures_df.rdd.getNumPartitions()

# Configure Spark to use 500 partitions
spark.conf.set('spark.sql.shuffle.partitions', 500)

# Recreate the DataFrame using the departures data file
departures_df = spark.read.csv('departures.txt.gz').distinct()

# Print the number of partitions for each instance
print("Partition count before change: %d" % before)
print("Partition count after change: %d" % departures_df.rdd.getNumPartitions())

-----------------------------------------------Normal joins--------------------------------------------------------------------------------------------------------------------------------

# Join the flights_df and aiports_df DataFrames
normal_df = flights_df.join(airports_df, \
    flights_df["Destination Airport"] == airports_df["IATA"] )

# Show the query plan
normal_df.explain()

------------------------------------------------Using broadcasting on Spark joins-------------------------------------------------------------------------------------
Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required 
and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.
You'll likely use broadcasting often with production datasets - checking the query plan will help validate your configuration without actually running the tasks

A couple tips:

Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.
On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.
If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting.

# Import the broadcast method from pyspark.sql.functions
from pyspark.sql.functions import broadcast

# Join the flights_df and aiports_df DataFrames using broadcasting
broadcast_df = flights_df.join(broadcast(airports_df), \
    flights_df["Destination Airport"] == airports_df["IATA"] )

# Show the query plan and compare against the original
broadcast_df.explain()

--------------------------------------------Comparing broadcast vs normal joins--------------------------------------------------------------------------

start_time = time.time()
# Count the number of rows in the normal DataFrame
normal_count = normal_df.count()
normal_duration = time.time() - start_time

start_time = time.time()
# Count the number of rows in the broadcast DataFrame
broadcast_count = broadcast_df.count()
broadcast_duration = time.time() - start_time

# Print the counts and the duration of the tests
print("Normal count:\t\t%d\tduration: %f" % (normal_count, normal_duration))
print("Broadcast count:\t%d\tduration: %f" % (broadcast_count, broadcast_duration))

-----------------------------------------------Quick pipeline----------------------------------------------------------------------------------------------------------

# Import the data to a DataFrame
departures_df = spark.read.csv('2015-departures.csv.gz', header=True)

# Remove any duration of 0
departures_df = departures_df.filter(departures_df[3] > 0)

# Add an ID column
departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())

# Write the file out to JSON format
departures_df.write.json('output.json')

-------------------------------------------------Pipeline data issue-----------------------------------------------------------------------------------------------------------
After creating your quick pipeline, you provide the json file to an analyst on your team. After loading the data and performing a couple exploratory tasks, 
the analyst tells you there's a problem in the dataset while trying to sort the duration data. 
She's not sure what the issue is beyond the sorting operation not working as expected.

Date          Flight Number   Airport     Duration    ID

09/30/2015    2287            ANC         409         107962
12/28/2015    1408            OKC         41          141917
08/11/2015    2287            ANC         410         87978
After analyzing the data, which command would fix the issue?

departures_df = departures_df.withColumn('Duration', departures_df['Duration'].cast(IntegerType()))

The original Duration column is a string, so when the analyst tries to sort the data, it's sorted alphabetically, not numerically. 
Remember that we could have also solved the issue using a defined schema during import to force specific types in the data.

--------------------------------------------------Removing commented lines-------------------------------------------------------------------------------------------------------------------------------
Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, 
but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, 
you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, 
allowing for quick analysis.

To start, you need to remove all commented rows in the dataset.

The spark context, and the base CSV file (annotations.csv.gz) are available for you to work with. The col function is also available for use.

# Import the file to a DataFrame and perform a row count
annotations_df = spark.read.csv('annotations.csv.gz', sep='|')
full_count = annotations_df.count()

# Count the number of rows beginning with '#'
comment_count = annotations_df.where(col('_c0').startswith('#')).count()

# Import the file to a new DataFrame, without commented rows
no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')

# Count the new DataFrame and verify the difference is as expected
no_comments_count = no_comments_df.count()
print("Full count: %d\nComment count: %d\nRemaining count: %d" % (full_count, comment_count, no_comments_count))

-------------------------------------------------------Removing invalid rows---------------------------------------------------------------------------------------------------------------------
Now that you've successfully removed the commented rows, you have received some information about the general format of the data. 
There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, 
so you'll need to split the data on the tab (\t) characters.

The DataFrame annotations_df is already available, with the commented rows removed. The spark.sql.functions library is available under the alias F. 
The initial number of rows available in the DataFrame is stored in the variable initial_count

# Split _c0 on the tab character and store the list in a variable
tmp_fields = F.split(annotations_df['_c0'], '\t')

# Create the colcount column on the DataFrame
annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))

# Remove any rows containing fewer than 5 fields
annotations_df_filtered = annotations_df.filter(~ (annotations_df["colcount"] < 5))

# Count the number of rows
final_count = annotations_df_filtered.count()
print("Initial count: %d\nFinal count: %d" % (initial_count, final_count))

------------------------------------------------------------Splitting into columns-------------------------------------------------------------------------------

You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations 
by generating specific meaningful columns based on the DataFrame content.

You have the spark context and the latest version of the annotations_df DataFrame. pyspark.sql.functions is available under the alias F

# Split the content of _c0 on the tab character (aka, '\t')
split_cols = F.split(annotations_df["_c0"], '\t')

# Add the columns folder, filename, width, and height
split_df = annotations_df.withColumn('folder', split_cols.getItem(0))
split_df = split_df.withColumn('filename', split_cols.getItem(1))
split_df = split_df.withColumn('width', split_cols.getItem(2))
split_df = split_df.withColumn('height', split_cols.getItem(3))

# Add split_cols as a column
split_df = split_df.withColumn('split_cols', split_cols)

In [1]: annotations_df.head(2)
Out[1]: 
[Row(_c0='02110627\tn02110627_12938\t200\t300\taffenpinscher,0,9,173,298', colcount=5),
 Row(_c0='02093754\tn02093754_1148\t500\t378\tBorder_terrier,73,127,341,335', colcount=5)]

In [2]: split_df.head(2)
Out[2]: 
[Row(_c0='02110627\tn02110627_12938\t200\t300\taffenpinscher,0,9,173,298', colcount=5, folder='02110627', filename='n02110627_12938', width='200', height='300', split_cols=['02110627', 'n02110627_12938', '200', '300', 'affenpinscher,0,9,173,298']),
 Row(_c0='02093754\tn02093754_1148\t500\t378\tBorder_terrier,73,127,341,335', colcount=5, folder='02093754', filename='n02093754_1148', width='500', height='378', split_cols=['02093754', 'n02093754_1148', '500', '378', 'Border_terrier,73,127,341,335'])]

--------------------------------------------------------Further parsing-------------------------------------------------------------------------------------------------------

You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. 
You need to prep the column data for use in later analysis and remove a few intermediary columns.

The spark context is available and pyspark.sql.functions is aliased as F. The types from pyspark.sql.types are already imported. 
The split_df DataFrame is as you last left it. Remember, you can use .printSchema() on a DataFrame in the console area to view the column names and types.

def retriever(cols, colcount):
  # Return a list of dog data
  return cols[4:colcount]

# Define the method as a UDF
udfRetriever = F.udf(retriever, ArrayType(StringType()))

# Create a new column using your UDF
split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))

# Remove the original column, split_cols, and the colcount
split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')

-----------------------------------------Validate rows via join----------------------------------------------------------------------------------------------------------------------------

Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named valid_folders_df. The DataFrame split_df is as you last left it with a group of split columns.

The spark object is available, and pyspark.sql.functions is imported as F.

# Rename the column in valid_folders_df
valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')

# Count the number of rows in split_df
split_count = split_df.count()

# Join the DataFrames
joined_df = split_df.join(F.broadcast(valid_folders_df), "folder")

# Compare the number of rows remaining
joined_count = joined_df.count()
print("Before: %d\nAfter: %d" % (split_count, joined_count))

---------------------------------------------Examining invalid rows------------------------------------------------------------------------------------------

You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources.

You want to find the difference between two DataFrames and store the invalid rows.

The spark object is defined and pyspark.sql.functions are imported as F. The original DataFrame split_df and the joined DataFrame joined_df are available as they were in their previous states.

# Determine the row counts for each DataFrame
split_count = split_df.count()
joined_count = joined_df.count()

# Create a DataFrame containing the invalid rows
invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')

# Validate the count of the new DataFrame is as expected
invalid_count = invalid_df.count()
print(" split_df:\t%d\n joined_df:\t%d\n invalid_df: \t%d" % (split_count, joined_count, invalid_count))

# Determine the number of distinct folder columns removed
invalid_folder_count = invalid_df.select('folder').distinct().count()
print("%d distinct invalid folders found" % invalid_folder_count)

---------------------------------------Dog parsing-------------------------------------------------------------------------------------------------------------------
You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. 
There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details.

The joined_df DataFrame is as you last defined it, and the pyspark.sql.types have all been imported.

# Select the dog details and show 10 untruncated rows
print(joined_df.select('dog_list').show(truncate=False))

# Define a schema type for the details in the dog list
DogType = StructType([
	StructField("breed", StringType(), False),
    StructField("start_x", IntegerType(), False),
    StructField("start_y", IntegerType(), False),
    StructField("end_x", IntegerType(), False),
    StructField("end_y", IntegerType(), False)
])

---------------------------------------------------------Per image count-----------------------------------------------------------------------------------
Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. 
You've been asked to calculate the number of dogs found in each image based on your dog_list column created earlier. 
You have also created the DogType which will allow better parsing of the data within some of the data columns.

The joined_df is available as you last defined it, and the DogType structtype is defined. pyspark.sql.functions is available under the F alias

# Create a function to return the number and type of dogs as a tuple
def dogParse(doglist):
  dogs = []
  for dog in doglist:
    (breed, start_x, start_y, end_x, end_y) = dog.split(',')
    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))
  return dogs

# Create a UDF
udfDogParse = F.udf(dogParse, ArrayType(DogType))

# Use the UDF to list of dogs and drop the old column
joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')

# Show the number of dogs in the first 10 rows
joined_df.select(F.size('dogs')).show(10)

-------------------------------------------Percentage dog pixels-------------------------------------------------------------------------------------------------------------

The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). 
You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.

To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. 
You can calculate the bounding box with the formula:

(Xend - Xstart) * (Yend - Ystart)

NOTE: You can ignore the possibility of overlapping bounding boxes in this instance.

For the percentage, calculate the total number of "dog" pixels divided by the total size of the image, multiplied by 100.
The joined_df DataFrame is as you last used it. pyspark.sql.functions is aliased to F

# Define a UDF to determine the number of pixels per image
def dogPixelCount(doglist):
  totalpixels = 0
  for dog in doglist:
    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])
  return totalpixels

# Define a UDF for the pixel count
udfDogPixelCount = F.udf(dogPixelCount, IntegerType())
joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))

# Create a column representing the percentage of pixels
joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)

# Show the first 10 annotations with more than 60% dog
joined_df.where('dog_percent > 60').show(10)

------------------------------------------------------------------------------------------------------





































----------------------------------------------------------------------------------------------------------------------------------------------------------------











