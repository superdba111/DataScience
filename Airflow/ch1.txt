repl:~$ airflow run etl_pipeline download_file 2020-01-08
[2020-08-06 16:02:18,363] {__init__.py:51} INFO - Using executor SequentialExecutor
[2020-08-06 16:02:18,683] {dagbag.py:90} INFO - Filling up the DagBag from /home/repl/workspace/dags
[2020-08-06 16:02:18,693] {cli.py:516} INFO - Running <TaskInstance: etl_pipeline.download_file 2020-01-08T00:00:00+00:00 [None]> on host 041f03ac-a57a-4166-a54c-2c5507ed336a.sessions.sessions.svc.cluster.local
[2020-08-06 16:02:19,299] {__init__.py:51} INFO - Using executor SequentialExecutor
[2020-08-06 16:02:19,621] {dagbag.py:90} INFO - Filling up the DagBag from /home/repl/workspace/dags/example_dag.py
[2020-08-06 16:02:19,633] {cli.py:516} INFO - Running <TaskInstance: etl_pipeline.download_file 2020-01-08T00:00:00+00:00 [None]> on host 041f03ac-a57a-4166-a54c-2c5507ed336a.sessions.sessions.svc.cluster.local
repl:~$ ls
airflow  config  start.sh  startup  workspace
repl:~$ pwd
/home/repl
repl:~$ 

repl:~$ ls airflow
airflow.cfg  airflow.db  airflow-webserver.pid  logs  unittests.cfg
repl:~$ ls config
config.sh
repl:~$ ls workspace
dags
repl:~$ ls workspace/dags
example_dag.py  __pycache__
repl:~$ ls startup
example.sh  start.sh
repl:~$

-----Defining a simple DAG
You've spent some time reviewing the Airflow components and are interested in testing out your own workflows. To start you decide to define the default arguments and create a DAG object for your workflow.

The DateTime object has been imported for you.


----
# Import the DAG object
from airflow.models import DAG

# Define the default_args dictionary
default_args = {
  'owner': 'dsmith',
  'start_date': datetime(2020, 1, 14),
  'retries': 2
}

# Instantiate the DAG object
etl_dag = DAG('example_etl', default_args=default_args)

----Working with DAGs and the Airflow shell
While working with Airflow, sometimes it can be tricky to remember what DAGs are defined and what they do. You want to gain some further knowledge of the Airflow shell command so you'd like to see what options are available.

Multiple DAGs are already defined for you. How many DAGs are present in the Airflow system from the command-line?

---airflow -h
repl:~$ airflow list_dags
[2020-08-06 16:12:18,708] {__init__.py:51} INFO - Using executor SequentialExecutor
[2020-08-06 16:12:19,137] {dagbag.py:90} INFO - Filling up the DagBag from /home/repl/workspace/dags


-------------------------------------------------------------------
DAGS
-------------------------------------------------------------------
example_dag
update_state


----------------Troubleshooting DAG creation
Now that you've successfully worked with a couple workflows, you notice that sometimes there are issues making a workflow appear within Airflow. You'd like to be able to better troubleshoot the behavior of Airflow when there may be something wrong with the code.

Two DAGs are defined for you and Airflow is setup. Note that any changes you make within the editor are automatically saved.

Use the airflow shell command to determine which DAG is not being recognized correctly.
After you determine the broken DAG, open the file and fix any Python errors.
Once modified, verify that the DAG now appears within Airflow's output.

----
repl:~/workspace$ airflow list_dags
[2020-08-06 16:14:20,081] {plugins_manager.py:148} ERROR - name '____' is not defined
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/airflow/plugins_manager.py", line 142, in <module>
    m = imp.load_source(namespace, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/repl/workspace/dags/refresh_data_workflow.py", line 1, in <module>
    ____
NameError: name '____' is not defined
[2020-08-06 16:14:20,153] {plugins_manager.py:149} ERROR - Failed to import plugin /home/repl/workspace/dags/refresh_data_workflow.py
[2020-08-06 16:14:20,163] {__init__.py:51} INFO - Using executor SequentialExecutor
[2020-08-06 16:14:20,483] {dagbag.py:90} INFO - Filling up the DagBag from /home/repl/workspace/dags


-------------------------------------------------------------------
DAGS
-------------------------------------------------------------------
etl_update

repl:~/workspace$ 




---sol,
repl:~/workspace$ vi /home/repl/workspace/dags/refresh_data_workflow.py


from airflow.models import DAG
default_args = {
  'owner': 'jdoe',
  'email': 'jdoe@datacamp.com'
}
dag = DAG( 'refresh_data', default_args=default_args )
repl:~/workspace$ 
repl:~/workspace$ 
repl:~/workspace$ 
repl:~/workspace$ 
repl:~/workspace$ airflow list_dags
[2020-08-06 16:19:36,867] {__init__.py:51} INFO - Using executor SequentialExecutor
[2020-08-06 16:19:37,186] {dagbag.py:90} INFO - Filling up the DagBag from /home/repl/workspace/dags


-------------------------------------------------------------------
DAGS
-------------------------------------------------------------------
etl_update
refresh_data


---------Starting the Airflow webserver
You've successfully created some DAGs within Airflow using the command-line tools, but notice that it can be a bit tricky to handle scheduling / troubleshooting / etc. After reading the documentation further, you realize that you'd like to access the Airflow web interface. For security reasons, you'd like to start the webserver on port 9090.

Which airflow command would you use to start the webserver on port 9090?

Airflow is installed and accessible from the command line. Remember to use the airflow -h command if needed. airflow <subcommand> -h will provide further detail

---sol,
airflow webserver -p 9090



---------Navigating the Airflow UI
To gain some familiarity with the Airflow UI, you decide to explore the various pages. You'd like to know what has happened on your Airflow instance thus far.

Which of the following events have not run on your Airflow instance?

---sol,cli_worker


--------Examining DAGs with the Airflow UI
You've become familiar with the basics of an Airflow DAG and the basics of interacting with Airflow on the command-line. Your boss would like you to show others on your team how to examine any available DAGs. In this instance, she would like to know which operator is NOT in use with the DAG called update_state, as your team is trying to verify the components used in production workflows.

Remember that the Airflow UI allows various methods to view the state of DAGs. The Tree View lists the tasks and any ordering between them in a tree structure, with the ability to compress / expand the nodes. The Graph View shows any tasks and their dependencies in a graph structure, along with the ability to access further details about task runs. The Code view provides full access to the Python code that makes up the DAG.

Remember to select the operator NOT used in this DAG.

----sol,
